{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.kaggle.com/williamroe/bi-lstm-with-crf-for-ner\n\nhttps://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/\n\nhttps://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede\n\nhttps://confusedcoders.com/data-science/deep-learning/how-to-build-deep-neural-network-for-custom-ner-with-keras\n\nhttps://www.kaggle.com/nikkisharma536/ner-with-bilstm-and-crf","metadata":{}},{"cell_type":"code","source":"!python --version\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nfrom wordcloud import WordCloud\n\nsns.set(font_scale = 1)\n%matplotlib inline\nplt.style.use('ggplot')\n\nfrom IPython.core.pylabtools import figsize\n\nimport nltk, re, string, collections\nfrom collections import Counter\n\nfrom nltk import word_tokenize \nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nfrom math import nan\n\nfrom future.utils import iteritems\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n!pip install git+https://www.github.com/keras-team/keras-contrib.git\nfrom keras_contrib.layers import CRF\n    \nfrom keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nimport keras as k\n\nfrom keras.callbacks import ModelCheckpoint\n\n!pip install seqeval\n\nfrom seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n\n!pip install sklearn_crfsuite\nfrom  sklearn_crfsuite.metrics import flat_classification_report  \n\nfrom keras.models import load_model\nfrom keras_contrib.utils import save_load_utils\nfrom keras.preprocessing import sequence\nimport re\n\nimport csv \n!pip install replaceall\nfrom replaceall import replaceall","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-15T05:17:44.752109Z","iopub.execute_input":"2021-06-15T05:17:44.752433Z","iopub.status.idle":"2021-06-15T05:18:27.580347Z","shell.execute_reply.started":"2021-06-15T05:17:44.752382Z","shell.execute_reply":"2021-06-15T05:18:27.579469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Formatting of Corpus**","metadata":{}},{"cell_type":"code","source":"# import csv \n\n# # reading dump file\n# ner_rows = []\n# sentence_id = 0\n\n# with open(\"../input/nercorpus/NER-corpus.DUMP\", \"r\") as ner_file:\n    \n#     for line in ner_file:\n#         label = line.split('\\t')[0]\n#         tags = line.split('\\t')[1].split()\n#         words = line.split('\\t')[2].split('\\n')[0].split()\n#         for tag, word in zip(tags, words):\n#             ner_rows.append([sentence_id, label, word.lower(), tag])\n        \n#         sentence_id+=1\n\n# # dump to csv for appropriate format\n# fields = ['sentence_id', 'label', 'word', 'tag']\n# filename = \"tbmm_ner.csv\"\n\n# with open(filename, 'w') as csvfile:\n#     csvwriter = csv.writer(csvfile)\n#     csvwriter.writerow(fields)\n#     csvwriter.writerows(ner_rows)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:18:27.58435Z","iopub.execute_input":"2021-06-15T05:18:27.584598Z","iopub.status.idle":"2021-06-15T05:18:27.591525Z","shell.execute_reply.started":"2021-06-15T05:18:27.584553Z","shell.execute_reply":"2021-06-15T05:18:27.590652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Loading of New Corpus**","metadata":{}},{"cell_type":"code","source":"ner_df = pd.read_csv(\"../input/tbmm-ner/tbmm_ner.csv\", encoding = \"utf-8\", error_bad_lines=False)\n# ffill: propagate last valid observation forward to next valid backfill # Fill NA/NaN values\nner_df = ner_df.fillna(method=\"ffill\")\nner_df.info()\nner_df.head(10)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-06-15T05:18:27.594341Z","iopub.execute_input":"2021-06-15T05:18:27.594568Z","iopub.status.idle":"2021-06-15T05:18:38.441927Z","shell.execute_reply.started":"2021-06-15T05:18:27.594524Z","shell.execute_reply":"2021-06-15T05:18:38.441091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Data Observe**","metadata":{}},{"cell_type":"code","source":"ner_words = [i.lower() for i in ner_df['word'] if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ]+$\", i) and len(i) > 2 and i not in stopwords.words('turkish')]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:18:38.443325Z","iopub.execute_input":"2021-06-15T05:18:38.443757Z","iopub.status.idle":"2021-06-15T05:25:49.597541Z","shell.execute_reply.started":"2021-06-15T05:18:38.44358Z","shell.execute_reply":"2021-06-15T05:25:49.596771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_words","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:25:49.600698Z","iopub.execute_input":"2021-06-15T05:25:49.60094Z","iopub.status.idle":"2021-06-15T05:25:49.617886Z","shell.execute_reply.started":"2021-06-15T05:25:49.600897Z","shell.execute_reply":"2021-06-15T05:25:49.616827Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = Counter(ner_words)\nc = list(c.most_common(500))\nmost_common = []\nfor i in range(len(c)):\n    most_common.append(c[i][0])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:25:49.741223Z","iopub.execute_input":"2021-06-15T05:25:49.741657Z","iopub.status.idle":"2021-06-15T05:25:50.947896Z","shell.execute_reply.started":"2021-06-15T05:25:49.741485Z","shell.execute_reply":"2021-06-15T05:25:50.947196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(background_color=\"white\",width=1000, height=600, max_font_size = 80).generate(' '.join(most_common))\nplt.figure(figsize=(40,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:25:50.949402Z","iopub.execute_input":"2021-06-15T05:25:50.949857Z","iopub.status.idle":"2021-06-15T05:25:52.144565Z","shell.execute_reply.started":"2021-06-15T05:25:50.94978Z","shell.execute_reply":"2021-06-15T05:25:52.14344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_df['tag'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:25:52.145724Z","iopub.execute_input":"2021-06-15T05:25:52.145979Z","iopub.status.idle":"2021-06-15T05:25:53.681824Z","shell.execute_reply.started":"2021-06-15T05:25:52.145941Z","shell.execute_reply":"2021-06-15T05:25:53.680524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figsize(20, 10)\nsns.countplot(ner_df['tag'], palette=\"colorblind\");\nplt.xlabel('Tags'); ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:25:53.68365Z","iopub.execute_input":"2021-06-15T05:25:53.684169Z","iopub.status.idle":"2021-06-15T05:25:56.346435Z","shell.execute_reply.started":"2021-06-15T05:25:53.683946Z","shell.execute_reply":"2021-06-15T05:25:56.345032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Data Formatting**","metadata":{}},{"cell_type":"code","source":"class SentenceGetter(object):\n    \n    def __init__(self, dataset):\n        self.n_sent = 1\n        self.dataset = dataset\n        self.empty = False\n        agg_func = lambda s: [(l, w, t) for l, w,t in zip(s[\"label\"].values.tolist(),\n                                                          s[\"word\"].values.tolist(),\n                                                          s[\"tag\"].values.tolist())]\n        self.grouped = self.dataset.groupby(\"sentence_id\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:25:56.348585Z","iopub.execute_input":"2021-06-15T05:25:56.349057Z","iopub.status.idle":"2021-06-15T05:25:56.366265Z","shell.execute_reply.started":"2021-06-15T05:25:56.348848Z","shell.execute_reply":"2021-06-15T05:25:56.365395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getter = SentenceGetter(ner_df)\nsentences = getter.sentences","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:25:56.367714Z","iopub.execute_input":"2021-06-15T05:25:56.368383Z","iopub.status.idle":"2021-06-15T05:27:01.788545Z","shell.execute_reply.started":"2021-06-15T05:25:56.368332Z","shell.execute_reply":"2021-06-15T05:27:01.787606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Seperating of Sentences**","metadata":{}},{"cell_type":"code","source":"labels = []\nfor label in set(ner_df[\"label\"].values):\n    if label is nan or isinstance(label, float):\n        labels.append('unk')\n    else:\n        labels.append(label)\nn_labels = len(labels)\n\nwords = list(set(ner_df[\"word\"].values))\nwords.append(\"unk\")\nn_words = len(words)\n\ntags = []\nfor tag in set(ner_df[\"tag\"].values):\n    if tag is nan or isinstance(tag, float):\n        tags.append('unk')\n    else:\n        tags.append(tag)\nn_tags = len(tags)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:27:01.790372Z","iopub.execute_input":"2021-06-15T05:27:01.79071Z","iopub.status.idle":"2021-06-15T05:27:03.473259Z","shell.execute_reply.started":"2021-06-15T05:27:01.790664Z","shell.execute_reply":"2021-06-15T05:27:03.472005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = max([len(s) for s in sentences])\nprint ('Maximum sentence length:', maxlen)\n\nprint ('The histogram of the lengths of sentences')\nplt.hist([len(s) for s in sentences], bins=50)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:27:03.475008Z","iopub.execute_input":"2021-06-15T05:27:03.475766Z","iopub.status.idle":"2021-06-15T05:27:04.17154Z","shell.execute_reply.started":"2021-06-15T05:27:03.475714Z","shell.execute_reply":"2021-06-15T05:27:04.170736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label2idx = {l: i for i, l in enumerate(labels)}\nword2idx = {w: i for i, w in enumerate(words)}\ntag2idx = {t: i for i, t in enumerate(tags)}\n\nidx2label = {v: k for k, v in iteritems(label2idx)}\nidx2word = {v: k for k, v in iteritems(word2idx)}\nidx2tag = {v: k for k, v in iteritems(tag2idx)}","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:27:04.173118Z","iopub.execute_input":"2021-06-15T05:27:04.173681Z","iopub.status.idle":"2021-06-15T05:27:04.448439Z","shell.execute_reply.started":"2021-06-15T05:27:04.173629Z","shell.execute_reply":"2021-06-15T05:27:04.447725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Data Splitting as Train and Test Sets**","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nX = [[word2idx[w[1]] for w in s] for s in sentences]\n# to ensure that all sequences in X have the same length\nX = pad_sequences(maxlen=maxlen, sequences=X, padding=\"post\",value=n_words - 1)\n\nfrom keras.utils import to_categorical\n\ny = [[tag2idx[w[2]] for w in s] for s in sentences]\n# to ensure that all sequences in y have the same length\ny = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n# we convert the y numpy array to a matrix which has binary values\ny = [to_categorical(i, num_classes=n_tags) for i in y]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:27:04.449847Z","iopub.execute_input":"2021-06-15T05:27:04.450143Z","iopub.status.idle":"2021-06-15T05:27:28.250861Z","shell.execute_reply.started":"2021-06-15T05:27:04.450099Z","shell.execute_reply":"2021-06-15T05:27:28.249993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:27:28.252334Z","iopub.execute_input":"2021-06-15T05:27:28.252622Z","iopub.status.idle":"2021-06-15T05:27:28.545184Z","shell.execute_reply.started":"2021-06-15T05:27:28.252578Z","shell.execute_reply":"2021-06-15T05:27:28.544404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://www.github.com/keras-team/keras-contrib.git","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:27:28.54683Z","iopub.execute_input":"2021-06-15T05:27:28.547297Z","iopub.status.idle":"2021-06-15T05:27:36.789375Z","shell.execute_reply.started":"2021-06-15T05:27:28.547115Z","shell.execute_reply":"2021-06-15T05:27:36.788387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X_train\", X_train[2])\nprint(\"X_test\", X_test[2])\n\nprint(\"y_train\", list(y_train[2]))\nprint(\"y_test\", list(y_test[2]))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:27:36.791121Z","iopub.execute_input":"2021-06-15T05:27:36.791434Z","iopub.status.idle":"2021-06-15T05:27:36.828315Z","shell.execute_reply.started":"2021-06-15T05:27:36.791366Z","shell.execute_reply":"2021-06-15T05:27:36.827528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **BI-LSTM and CRF Model & Traning of Model**","metadata":{}},{"cell_type":"code","source":"from keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nimport keras as k\nfrom keras_contrib.layers import CRF\n\ninput = Input(shape=(50,))\nword_embedding_size = 300\n\n# Embedding Layer\nmodel = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=50)(input)\n\n# BI-LSTM Layer\nmodel = Bidirectional(LSTM(units=word_embedding_size, \n                           return_sequences=True, \n                           dropout=0.5, \n                           recurrent_dropout=0.5, \n                           kernel_initializer=k.initializers.he_normal()))(model)\nmodel = LSTM(units=word_embedding_size * 2, \n             return_sequences=True, \n             dropout=0.5, \n             recurrent_dropout=0.5, \n             kernel_initializer=k.initializers.he_normal())(model)\n\n# TimeDistributed Layer\nmodel = TimeDistributed(Dense(n_tags, activation=\"relu\"))(model)  \n\n# CRF Layer\ncrf = CRF(n_tags)\nout = crf(model)\n\nmodel = Model(input, out)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:27:36.829778Z","iopub.execute_input":"2021-06-15T05:27:36.830055Z","iopub.status.idle":"2021-06-15T05:27:41.125802Z","shell.execute_reply.started":"2021-06-15T05:27:36.829963Z","shell.execute_reply":"2021-06-15T05:27:41.125004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\n# Optimiser \nadam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n\n# Compile model\nmodel.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n\nmodel.summary()\n\n# Saving the best model only\nfilepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\n# Fit the best model\nhistory = model.fit(X_train, np.array(y_train), batch_size=256, epochs=10, validation_split=0.1, verbose=1, callbacks=callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:27:41.127108Z","iopub.execute_input":"2021-06-15T05:27:41.127389Z","iopub.status.idle":"2021-06-15T06:27:17.802511Z","shell.execute_reply.started":"2021-06-15T05:27:41.127335Z","shell.execute_reply":"2021-06-15T06:27:17.801562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Evaluation of the Results**","metadata":{}},{"cell_type":"code","source":"# Plot the graph \nplt.style.use('ggplot')\n\ndef plot_history(history):\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(accuracy) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, accuracy, 'b', label='Training acc')\n    plt.plot(x, val_accuracy, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\nplot_history(history)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:27:17.803913Z","iopub.execute_input":"2021-06-15T06:27:17.804228Z","iopub.status.idle":"2021-06-15T06:27:18.276908Z","shell.execute_reply.started":"2021-06-15T06:27:17.804177Z","shell.execute_reply":"2021-06-15T06:27:18.275902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred2label(pred):\n    out = []\n    for pred_i in pred:\n        out_i = []\n        for p in pred_i:\n            p_i = np.argmax(p)\n            out_i.append(idx2label[p_i])\n            out_i.append(idx2word[p_i])\n            out_i.append(idx2tag[p_i])\n            \n        out.append(out_i)\n    return out\n\ntest_pred = model.predict(X_test, verbose=1)   \npred_labels = pred2label(test_pred)\ntest_labels = pred2label(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:27:18.278722Z","iopub.execute_input":"2021-06-15T06:27:18.279229Z","iopub.status.idle":"2021-06-15T06:32:14.241356Z","shell.execute_reply.started":"2021-06-15T06:27:18.279076Z","shell.execute_reply":"2021-06-15T06:32:14.240447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install seqeval\n# from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n\n# print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:32:14.242691Z","iopub.execute_input":"2021-06-15T06:32:14.243014Z","iopub.status.idle":"2021-06-15T06:32:14.247427Z","shell.execute_reply.started":"2021-06-15T06:32:14.242936Z","shell.execute_reply":"2021-06-15T06:32:14.246317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sklearn_crfsuite\nfrom  sklearn_crfsuite.metrics import flat_classification_report \n\nreport = flat_classification_report(y_pred=pred_labels, y_true=test_labels)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:32:14.249395Z","iopub.execute_input":"2021-06-15T06:32:14.249878Z","iopub.status.idle":"2021-06-15T06:40:11.764902Z","shell.execute_reply.started":"2021-06-15T06:32:14.249677Z","shell.execute_reply":"2021-06-15T06:40:11.763852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP = {}\nTN = {}\nFP = {}\nFN = {}\nfor tag in tag2idx.keys():\n    TP[tag] = 0\n    TN[tag] = 0    \n    FP[tag] = 0    \n    FN[tag] = 0    \n\ndef accumulate_score_by_tag(gt, pred):\n    \"\"\"\n    For each tag keep stats\n    \"\"\"\n    if gt == pred:\n        TP[gt] += 1\n    elif gt != 'O' and pred == 'O':\n        FN[gt] +=1\n    elif gt == 'O' and pred != 'O':\n        FP[gt] += 1\n    else:\n        TN[gt] += 1","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:48:13.414711Z","iopub.execute_input":"2021-06-15T06:48:13.415052Z","iopub.status.idle":"2021-06-15T06:48:13.423294Z","shell.execute_reply.started":"2021-06-15T06:48:13.414994Z","shell.execute_reply":"2021-06-15T06:48:13.422418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, sentence in enumerate(X_test):\n    y_hat = np.argmax(test_pred[0], axis=-1)\n    gt = np.argmax(y_test[0], axis=-1)\n    for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n        accumulate_score_by_tag(idx2tag[gt[idx]],tags[pred])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:48:42.763145Z","iopub.execute_input":"2021-06-15T06:48:42.763564Z","iopub.status.idle":"2021-06-15T06:48:57.92021Z","shell.execute_reply.started":"2021-06-15T06:48:42.763494Z","shell.execute_reply":"2021-06-15T06:48:57.919318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tag in tag2idx.keys():\n    print(f'tag:{tag}')    \n    print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n    print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))    ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:40:27.102937Z","iopub.execute_input":"2021-06-15T06:40:27.1033Z","iopub.status.idle":"2021-06-15T06:40:27.11242Z","shell.execute_reply.started":"2021-06-15T06:40:27.103237Z","shell.execute_reply":"2021-06-15T06:40:27.110576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 357\np = model.predict(np.array([X_test[i]]))\np = np.argmax(p, axis=-1)\ngt = np.argmax(y_test[i], axis=-1)\nprint(\"{:14}: {:5}: {}\".format(\"Word\\t\", \"True\\t\", \"Pred\\t\"))\nfor idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n    if words[w] != \"unk\":\n        print(\"{:14}: {:5}: \\t{}\".format(words[w],idx2tag[gt[idx]],tags[pred]))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:40:27.120324Z","iopub.execute_input":"2021-06-15T06:40:27.120874Z","iopub.status.idle":"2021-06-15T06:40:27.186129Z","shell.execute_reply.started":"2021-06-15T06:40:27.120775Z","shell.execute_reply":"2021-06-15T06:40:27.185162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Demo Screen**","metadata":{}},{"cell_type":"code","source":"# reading demo file\ntbmm_text = []\nwith open(\"../input/tbmmcorpusdonem2027/NLP/donem22/yıl3/10.txt\", \"r\",encoding = \"utf-8\") as file:\n    \n    for line in file:\n        line = replaceall(line, \"\\n\")\n        tbmm_text.append([i.lower() for i in line.split(\" \") if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ.,:]+$\", i) and len(i) > 1 and i not in stopwords.words('turkish')])\n        \nprint(len(tbmm_text))\n\n# formatting for ner  \ndemo_df = []\nsentence_id = 0\nfor i in tbmm_text:\n    for j in i:\n        demo_df.append([sentence_id, \"unk\", j, \"unk\"])\n    sentence_id+=1\n\ndemo_df = pd.DataFrame(demo_df)\ndemo_df.columns =['sentence_id','label','word','tag']\ndemo_df.info()\ndemo_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T07:10:05.804517Z","iopub.execute_input":"2021-06-15T07:10:05.804811Z","iopub.status.idle":"2021-06-15T07:10:06.512191Z","shell.execute_reply.started":"2021-06-15T07:10:05.804762Z","shell.execute_reply":"2021-06-15T07:10:06.51152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getter_demo = SentenceGetter(demo_df)\nsentences_demo = getter_demo.sentences\n\nwords_demo = list(set(demo_df[\"word\"].values))\nwords_demo.append(\"unk\")\nn_words_demo = len(words_demo)\n\nword2idx_demo = {w: i for i, w in enumerate(words_demo)}\nidx2word_demo = {v: k for k, v in iteritems(word2idx_demo)}\n\nX_demo = [[word2idx_demo[w[1]] for w in s] for s in sentences_demo]\n# to ensure that all sequences in X have the same length\nX_demo = pad_sequences(maxlen=maxlen, sequences=X_demo, padding=\"post\",value=n_words_demo - 1)\n\nchk = []\nfor i in sentences:\n    chk.append([i[0][1], i[0][2]])\n    \nfrom prettytable import PrettyTable\ntable = PrettyTable(border=True, header=True, padding_width=1)\ntable.field_names = ['Word', 'Prediction', 'Checking']\n\ndef checking(word):\n    for i in chk:\n        if i[0] == word:\n            return i[1]\n    return \"unk\"\n\nfor i in range(len(X_demo)):\n    p = model.predict(np.array([X_demo[i]]))\n    p = np.argmax(p, axis=-1)\n    for idx,(w,pred) in enumerate(zip(X_demo[i],p[0])):\n        if words_demo[w] != \"unk\":\n            print(words_demo[w],tags[pred])\n#             table.add_row([words_demo[w],tags[pred],checking(words_demo[w])])\n    i+=1\n# print(table)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T07:10:20.760954Z","iopub.execute_input":"2021-06-15T07:10:20.761313Z","iopub.status.idle":"2021-06-15T07:11:34.888731Z","shell.execute_reply.started":"2021-06-15T07:10:20.761256Z","shell.execute_reply":"2021-06-15T07:11:34.887774Z"},"trusted":true},"execution_count":null,"outputs":[]}]}