{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python --version\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "sns.set(font_scale = 1)\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "import nltk, re, string, collections\n",
    "from collections import Counter\n",
    "\n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from math import nan\n",
    "\n",
    "from future.utils import iteritems\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "from keras_contrib.layers import CRF\n",
    "    \n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "import keras as k\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "!pip install seqeval\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "!pip install sklearn_crfsuite\n",
    "from  sklearn_crfsuite.metrics import flat_classification_report  \n",
    "\n",
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "from keras.models import load_model\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "import keras as k\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.preprocessing import sequence\n",
    "import re\n",
    "from future.utils import iteritems\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# import csv \n",
    "\n",
    "# # reading dump file\n",
    "# ner_rows = []\n",
    "# sentence_id = 0\n",
    "\n",
    "# with open(\"../input/nercorpus/NER-corpus.DUMP\", \"r\") as ner_file:\n",
    "    \n",
    "#     for line in ner_file:\n",
    "#         label = line.split('\\t')[0]\n",
    "#         tags = line.split('\\t')[1].split()\n",
    "#         words = line.split('\\t')[2].split('\\n')[0].split()\n",
    "#         for tag, word in zip(tags, words):\n",
    "#             ner_rows.append([sentence_id, label, word.lower(), tag])\n",
    "        \n",
    "#         sentence_id+=1\n",
    "\n",
    "# # dump to csv for appropriate format\n",
    "# fields = ['sentence_id', 'label', 'word', 'tag']\n",
    "# filename = \"tbmm_ner.csv\"\n",
    "\n",
    "# with open(filename, 'w') as csvfile:\n",
    "#     csvwriter = csv.writer(csvfile)\n",
    "#     csvwriter.writerow(fields)\n",
    "#     csvwriter.writerows(ner_rows)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ner_df = pd.read_csv(\"../input/tbmm-ner/tbmm_ner.csv\", encoding = \"utf-8\", error_bad_lines=False)\n",
    "ner_df = ner_df.fillna(method=\"ffill\")\n",
    "ner_df.info()\n",
    "ner_df.head(10)"
   ],
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# > **Data Observe**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "ner_words = [i.lower() for i in ner_df['word'] if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ]+$\", i) and len(i) > 1 and i not in stopwords.words('turkish')]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "c = Counter(ner_words)\n",
    "c = list(c.most_common(500))\n",
    "most_common = []\n",
    "for i in range(len(c)):\n",
    "    most_common.append(c[i][0])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wordcloud = WordCloud(background_color=\"white\",width=1000, height=600, max_font_size = 80).generate(' '.join(most_common))\n",
    "plt.figure(figsize=(40,10), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ner_df['tag'].value_counts()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "figsize(20, 10)\n",
    "sns.countplot(ner_df['tag'], palette=\"colorblind\");\n",
    "plt.xlabel('Tags'); "
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# > **Data Formatting**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(l, w, t) for l, w,t in zip(s[\"label\"].values.tolist(),\n",
    "                                                          s[\"word\"].values.tolist(),\n",
    "                                                          s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby(\"sentence_id\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "           \n",
    "getter = SentenceGetter(ner_df)\n",
    "sentences = getter.sentences"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labels = []\n",
    "for label in set(ner_df[\"label\"].values):\n",
    "    if label is nan or isinstance(label, float):\n",
    "        labels.append('unk')\n",
    "    else:\n",
    "        labels.append(label)\n",
    "n_labels = len(labels)\n",
    "\n",
    "words = list(set(ner_df[\"word\"].values))\n",
    "words.append(\"unk\")\n",
    "n_words = len(words)\n",
    "\n",
    "tags = []\n",
    "for tag in set(ner_df[\"tag\"].values):\n",
    "    if tag is nan or isinstance(tag, float):\n",
    "        tags.append('unk')\n",
    "    else:\n",
    "        tags.append(tag)\n",
    "n_tags = len(tags)\n",
    "print(len(words))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "maxlen = max([len(s) for s in sentences])\n",
    "print ('Maximum sentence length:', maxlen)\n",
    "\n",
    "print ('The histogram of the lengths of sentences')\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "label2idx = {l: i for i, l in enumerate(labels)}\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "idx2label = {v: k for k, v in iteritems(label2idx)}\n",
    "idx2word = {v: k for k, v in iteritems(word2idx)}\n",
    "idx2tag = {v: k for k, v in iteritems(tag2idx)}"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# > **Data Splitting as Train and Test Sets**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X = [[word2idx[w[1]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=maxlen, sequences=X, padding=\"post\",value=n_words - 1)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(sentences[2])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"X_train\", X_train[2])\n",
    "print(\"X_test\", X_test[2])\n",
    "\n",
    "print(\"y_train\", list(y_train[2]))\n",
    "print(\"y_test\", list(y_test[2]))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# > **BI-LSTM and CRF Model & Traning of Model**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "import keras as k\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "input = Input(shape=(50,))\n",
    "word_embedding_size = 300\n",
    "\n",
    "# Embedding Layer\n",
    "model = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=50)(input)\n",
    "\n",
    "# BI-LSTM Layer\n",
    "model = Bidirectional(LSTM(units=word_embedding_size, \n",
    "                           return_sequences=True, \n",
    "                           dropout=0.5, \n",
    "                           recurrent_dropout=0.5, \n",
    "                           kernel_initializer=k.initializers.he_normal()))(model)\n",
    "model = LSTM(units=word_embedding_size * 2, \n",
    "             return_sequences=True, \n",
    "             dropout=0.5, \n",
    "             recurrent_dropout=0.5, \n",
    "             kernel_initializer=k.initializers.he_normal())(model)\n",
    "\n",
    "# TimeDistributed Layer\n",
    "model = TimeDistributed(Dense(n_tags, activation=\"relu\"))(model)  \n",
    "\n",
    "# CRF Layer\n",
    "crf = CRF(n_tags)\n",
    "out = crf(model)\n",
    "\n",
    "model = Model(input, out)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Optimiser \n",
    "adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Saving the best model only\n",
    "filepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit the best model\n",
    "history = model.fit(X_train, np.array(y_train), batch_size=256, epochs=20, validation_split=0.1, verbose=1, callbacks=callbacks_list)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# > **Evaluation of the Results**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the graph \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(accuracy) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, accuracy, 'b', label='Training acc')\n",
    "    plt.plot(x, val_accuracy, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "plot_history(history)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2label[p_i])\n",
    "            out_i.append(idx2word[p_i])\n",
    "            out_i.append(idx2tag[p_i])\n",
    "            \n",
    "        out.append(out_i)\n",
    "    return out\n",
    "\n",
    "test_pred = model.predict(X_test, verbose=1)   \n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = pred2label(y_test)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install seqeval\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install sklearn_crfsuite\n",
    "from  sklearn_crfsuite.metrics import flat_classification_report \n",
    "\n",
    "report = flat_classification_report(y_pred=pred_labels, y_true=test_labels)\n",
    "print(report)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TP = {}\n",
    "TN = {}\n",
    "FP = {}\n",
    "FN = {}\n",
    "for tag in tag2idx.keys():\n",
    "    TP[tag] = 0\n",
    "    TN[tag] = 0    \n",
    "    FP[tag] = 0    \n",
    "    FN[tag] = 0    \n",
    "\n",
    "def accumulate_score_by_tag(gt, pred):\n",
    "    \"\"\"\n",
    "    For each tag keep stats\n",
    "    \"\"\"\n",
    "    if gt == pred:\n",
    "        TP[gt] += 1\n",
    "    elif gt != 'O' and pred == 'O':\n",
    "        FN[gt] +=1\n",
    "    elif gt == 'O' and pred != 'O':\n",
    "        FP[gt] += 1\n",
    "    else:\n",
    "        TN[gt] += 1"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i, sentence in enumerate(X_test):\n",
    "    y_hat = np.argmax(test_pred[0], axis=-1)\n",
    "    gt = np.argmax(y_test[0], axis=-1)\n",
    "    for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n",
    "        accumulate_score_by_tag(idx2tag[gt[idx]],tags[pred])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for tag in tag2idx.keys():\n",
    "    print(f'tag:{tag}')    \n",
    "    print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n",
    "    print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))    "
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "i = 357\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "gt = np.argmax(y_test[i], axis=-1)\n",
    "print(\"{:14}: {:5}: {}\".format(\"Word\\t\", \"True\\t\", \"Pred\\t\"))\n",
    "for idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n",
    "    if words[w] != \"unk\":\n",
    "        print(\"{:14}: {:5}: \\t{}\".format(words[w],idx2tag[gt[idx]],tags[pred]))"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}