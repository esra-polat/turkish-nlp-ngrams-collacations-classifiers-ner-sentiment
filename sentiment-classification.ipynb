{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import keras\n\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM,  Dropout\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re\nfrom keras.models import load_model\n\nimport csv\nfrom csv import writer\nfrom csv import DictWriter\n\nimport glob\n\nimport nltk, re, string, collections\nfrom nltk import word_tokenize \nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-23T03:00:53.056315Z","iopub.execute_input":"2021-06-23T03:00:53.056598Z","iopub.status.idle":"2021-06-23T03:00:59.255357Z","shell.execute_reply.started":"2021-06-23T03:00:53.056533Z","shell.execute_reply":"2021-06-23T03:00:59.254136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_corpus = []","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:00:59.257018Z","iopub.execute_input":"2021-06-23T03:00:59.257498Z","iopub.status.idle":"2021-06-23T03:00:59.261695Z","shell.execute_reply.started":"2021-06-23T03:00:59.257456Z","shell.execute_reply":"2021-06-23T03:00:59.260885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_comments = pd.read_csv(\"../input/productcomments/data.csv\", encoding = \"utf-8\", error_bad_lines=False)\n\nfor y,d in zip(product_comments['Yorum'],product_comments['Duygu']):\n    y = [i.lower() for i in y.split() if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ]+$\", i)]\n    sentiment_corpus.append([y, d])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:00:59.264024Z","iopub.execute_input":"2021-06-23T03:00:59.264602Z","iopub.status.idle":"2021-06-23T03:00:59.388128Z","shell.execute_reply.started":"2021-06-23T03:00:59.264565Z","shell.execute_reply":"2021-06-23T03:00:59.387303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/sa-imdb/SA_IMDB_translated_TR/TEST/negTr/*.txt\"\nall_files = glob.glob(path)\nfor file in all_files:\n    lines = open(file).readlines()\n    for l in lines:\n        l = [i.lower() for i in l.split() if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ]+$\", i)]\n        sentiment_corpus.append([l, 0])\n        \npath = \"../input/sa-imdb/SA_IMDB_translated_TR/TEST/posTr/*.txt\"\nall_files = glob.glob(path)\nfor file in all_files:\n    lines = open(file).readlines()\n    for l in lines:\n        l = [i.lower() for i in l.split() if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ]+$\", i)]\n        sentiment_corpus.append([l, 1])\n\npath = \"../input/sa-imdb/SA_IMDB_translated_TR/TRAIN/negTr/*.txt\"\nall_files = glob.glob(path)\nfor file in all_files:\n    lines = open(file).readlines()\n    for l in lines:\n        l = [i.lower() for i in l.split() if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ]+$\", i)]\n        sentiment_corpus.append([l, 0])\n\npath = \"../input/sa-imdb/SA_IMDB_translated_TR/TRAIN/posTr/*.txt\"\nall_files = glob.glob(path)\nfor file in all_files:\n    lines = open(file).readlines()\n    for l in lines:\n        l = [i.lower() for i in l.split() if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ]+$\", i)]\n        sentiment_corpus.append([l, 1])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:00:59.392242Z","iopub.execute_input":"2021-06-23T03:00:59.394207Z","iopub.status.idle":"2021-06-23T03:05:55.705602Z","shell.execute_reply.started":"2021-06-23T03:00:59.394167Z","shell.execute_reply":"2021-06-23T03:05:55.70474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_reviews = open(\"../input/sa-tr-reviews/reviews.neg\").readlines()\nfor x in tr_reviews:\n    x = [i.lower() for i in x.split() if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ]+$\", i)]\n    sentiment_corpus.append([x, 0])\n\ntr_reviews = open(\"../input/sa-tr-reviews/reviews.pos\").readlines()\nfor x in tr_reviews:\n    x = [i.lower() for i in x.split() if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ]+$\", i)]\n    sentiment_corpus.append([x, 1])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:05:55.707062Z","iopub.execute_input":"2021-06-23T03:05:55.707405Z","iopub.status.idle":"2021-06-23T03:06:05.755771Z","shell.execute_reply.started":"2021-06-23T03:05:55.707369Z","shell.execute_reply":"2021-06-23T03:06:05.754822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_array =np.array(sentiment_corpus)\ndf = pd.DataFrame(my_array, columns = ['text','sentiment'])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:05.757047Z","iopub.execute_input":"2021-06-23T03:06:05.757371Z","iopub.status.idle":"2021-06-23T03:06:07.347435Z","shell.execute_reply.started":"2021-06-23T03:06:05.757336Z","shell.execute_reply":"2021-06-23T03:06:07.346258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:07.348587Z","iopub.execute_input":"2021-06-23T03:06:07.348922Z","iopub.status.idle":"2021-06-23T03:06:07.372708Z","shell.execute_reply.started":"2021-06-23T03:06:07.348892Z","shell.execute_reply":"2021-06-23T03:06:07.371622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(\"../input/sentiment-analysis-big-corpus/sentiment-analysis-bigcorpus.csv\", encoding = \"utf-8\", error_bad_lines=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:07.376513Z","iopub.execute_input":"2021-06-23T03:06:07.376763Z","iopub.status.idle":"2021-06-23T03:06:07.380073Z","shell.execute_reply.started":"2021-06-23T03:06:07.376738Z","shell.execute_reply":"2021-06-23T03:06:07.378998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:07.38217Z","iopub.execute_input":"2021-06-23T03:06:07.382777Z","iopub.status.idle":"2021-06-23T03:06:07.389048Z","shell.execute_reply.started":"2021-06-23T03:06:07.382742Z","shell.execute_reply":"2021-06-23T03:06:07.388317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.DataFrame(df, columns = ['text','sentiment'])\n# # ","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:07.390353Z","iopub.execute_input":"2021-06-23T03:06:07.390716Z","iopub.status.idle":"2021-06-23T03:06:07.398882Z","shell.execute_reply.started":"2021-06-23T03:06:07.39068Z","shell.execute_reply":"2021-06-23T03:06:07.398104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:07.400105Z","iopub.execute_input":"2021-06-23T03:06:07.400528Z","iopub.status.idle":"2021-06-23T03:06:07.464063Z","shell.execute_reply.started":"2021-06-23T03:06:07.400443Z","shell.execute_reply":"2021-06-23T03:06:07.463081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df[df['sentiment'] == 1].size)\nprint(df[df['sentiment'] == 0].size)\n    \nmax_fatures = 236141\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(df['text'].values)\nX = tokenizer.texts_to_sequences(df['text'].values)\nX = pad_sequences(X)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:07.465523Z","iopub.execute_input":"2021-06-23T03:06:07.466076Z","iopub.status.idle":"2021-06-23T03:06:24.465767Z","shell.execute_reply.started":"2021-06-23T03:06:07.466042Z","shell.execute_reply":"2021-06-23T03:06:24.464962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:24.467006Z","iopub.execute_input":"2021-06-23T03:06:24.467328Z","iopub.status.idle":"2021-06-23T03:06:26.810287Z","shell.execute_reply.started":"2021-06-23T03:06:24.467295Z","shell.execute_reply":"2021-06-23T03:06:26.809485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nY = pd.get_dummies(df['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:26.811572Z","iopub.execute_input":"2021-06-23T03:06:26.811911Z","iopub.status.idle":"2021-06-23T03:06:27.068074Z","shell.execute_reply.started":"2021-06-23T03:06:26.811875Z","shell.execute_reply":"2021-06-23T03:06:27.066716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # load model\n# model = load_model('../input/lstm-model/lstm_sa_model (1).h5')\n# # summarize model.\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:27.069329Z","iopub.execute_input":"2021-06-23T03:06:27.069673Z","iopub.status.idle":"2021-06-23T03:06:27.073777Z","shell.execute_reply.started":"2021-06-23T03:06:27.069636Z","shell.execute_reply":"2021-06-23T03:06:27.072845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, Y_train, epochs = 10, batch_size=256, verbose = 1)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T03:06:27.075195Z","iopub.execute_input":"2021-06-23T03:06:27.075818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('lstm_sa_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nvalidation_size = 1500\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = 256)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    \n    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=256,verbose = 0)[0]\n   \n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(Y_validate[x]) == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\n\n\nprint(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(\"../input/tbmmcorpusdonem2027/NLP/donem25/yıl2/1.txt\")\n\ndemo = [\"Domates üreticilerinin ürünlerinin satışında yaşadıkları sorunlara, İstanbul Milletvekili Emine Beyza Üstün'ün cevabı.\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"../input/tbmmcorpusdonem2027/NLP/donem20/yıl1/44.txt\\n\")\n\ndemo = [\"Söz konusu davadan vazgeçilmesi halinde İstanbul Bankasına ait tüm borçların hazine tarafından karşılanması kesinlikle söz konusu değildir.\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"../input/tbmmcorpusdonem2027/NLP/donem21/yıl3/124.txt\\n\")\n\ndemo = [\"Amaç, millete hizmet değil mi?! Biz, bu kararla, en azından, önemli bir şeyin herkes tarafından anlaşılmış olmasından dolayı memnunuz.\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"../input/tbmmcorpusdonem2027/NLP/donem22/yıl5/119.txt\\n\")\n\ndemo = [\" Efendim, İç Tüzük'ün 69'uncu maddesine göre, Sayın Hatip Anavatan Grubunu suçlayarak söze başladığı için, söz almak istiyorum.\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"../input/tbmmcorpusdonem2027/NLP/donem23/yıl5/24.txt\\n\")\n\ndemo = [\"Türkiye Büyük Millet Meclisinin 90’ıncı yılında, Türk kadınının belediye meclislerine seçme ve seçilme hakkını kazanışının 80’inci yılında ve milletvekili seçme ve seçilme hakkını elde edişinin 76’ncı yılında durum kadınlar ve demokrasimiz açısından vahimdir.\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=30, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"../input/tbmmcorpusdonem2027/NLP/donem24/yıl2/118.txt\\n\")\n\ndemo = [\"Milletvekili Adnan Keskin’in, Denizli’de dolu nedeniyle üzüm bağlarının uğradığı zarara ve üreticilerin mağduriyetine ilişkin Gıda, Tarım ve Hayvancılık Bakanından yazılı soru önergesi\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=30, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"../input/tbmmcorpusdonem2027/NLP/donem25/yıl2/1.txt\\n\")\n\ndemo = [\"Domates üreticilerinin ürünlerinin satışında yaşadıkları sorunlara, İstanbul Milletvekili Emine Beyza Üstün'ün cevabı.\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"../input/tbmmcorpusdonem2027/NLP/donem26/yıl2/15.txt\\n\")\n\ndemo = [\"Bursa Milletvekili Orhan Sarıbal’ın, hiçbir zaman bu kadar faşizan, bu kadar baskıcı bir düzen görülmediğine ve basına, insan hayatına bu kadar müdahalenin olmadığına ilişkin açıklaması\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=30, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"../input/tbmmcorpusdonem2027/NLP/donem27/yıl4/17.txt\\n\")\n\ndemo = [\"Daha niceleri iktidarın baskı ve korkusuna direnemeyen hâkim ve savcılar yüzünden hâlen cezaevindedir.\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo = [\"Bu bir demo cümlesidir. Bakalım sonuç ne olacak?\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo = [\"Tüm bu olanlar için çok üzgünüm.\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo = [\"Gerçekten çok sevindim, kutlarım!\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo = [\"Ben bu üründen hiç bir şey anlamadım iyi mi kötü mü bilemedim.\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo = [\"Bu günlerde çok kararsızım.\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=25, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo = [\"Bu cümleden beklentim pozitif yönde bir sonuç vermesi. Kafa karışıklığı için üzgün olduğumu belirtmek istiyorum. Mesela bu ürün gerçekten berbat desem sonucu etkiler miyim? Bakalım!\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=30, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo = [\"Bu cümleden beklentim negatif yönde bir sonuç vermesi. Kafa karışıklığı için mutlu olduğumu belirtmek istiyorum. Mesela bu ürün gerçekten harika desem sonucu etkiler miyim? Bakalım!\"]\nprint(demo)\ndemo = tokenizer.texts_to_sequences(demo)\ndemo = pad_sequences(demo, maxlen=30, dtype='int32', value=0)\nprint(demo)\nsentiment = model.predict(demo,batch_size=256,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}